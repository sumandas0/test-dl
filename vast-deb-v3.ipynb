{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install seqeval evaluate rjieba wandb pytorch-lightning transformers==4.37.2 datasets==2.17.0 sentensepiece -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from datasets import Dataset as DS\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    AutoTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    pretrained_model_name = \"microsoft/deberta-v3-large\"\n",
    "    training_max_length = 512\n",
    "    base_path = \"./\"\n",
    "    output_dir = base_path+\"output\"\n",
    "    ds_path  = base_path+\"/train.json\"\n",
    "    seed = 42\n",
    "    batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds_path = \".\"\n",
    "data = json.load(open(f\"{ds_path}/train.json\"))\n",
    "\n",
    "# downsampling of negative examples\n",
    "p=[] # positive samples (contain relevant labels)\n",
    "n=[] # negative samples (presumably contain entities that are possibly wrongly classified as entity)\n",
    "for d in data:\n",
    "    if any(np.array(d[\"labels\"]) != \"O\"): p.append(d)\n",
    "    else: n.append(d)\n",
    "print(\"original datapoints: \", len(data))\n",
    "\n",
    "external = json.load(open(f\"{ds_path}/pii_dataset_fixed.json\"))\n",
    "print(\"external datapoints: \", len(external))\n",
    "\n",
    "moredata = json.load(open(f\"{ds_path}/moredata_dataset_fixed.json\"))\n",
    "print(\"moredata datapoints: \", len(moredata))\n",
    "\n",
    "data = external+moredata+p+n[:len(n)//3]\n",
    "print(\"combined: \", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "target = [\n",
    "    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n",
    "    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n",
    "    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n",
    "]\n",
    "\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer, label2id):\n",
    "    text = []\n",
    "\n",
    "    # these are at the character level\n",
    "    labels = []\n",
    "    targets = []\n",
    "\n",
    "    for t, l, ws in zip(example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]):\n",
    "\n",
    "        text.append(t)\n",
    "        labels.extend([l]*len(t))\n",
    "        \n",
    "        if l in target:\n",
    "            targets.append(1)\n",
    "        else:\n",
    "            targets.append(0)\n",
    "        # if there is trailing whitespace\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            labels.append(\"O\")\n",
    "\n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=CFG.training_max_length)\n",
    "    \n",
    "    target_num = sum(targets)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    text = \"\".join(text)\n",
    "    token_labels = []\n",
    "\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "\n",
    "        # CLS token\n",
    "        if start_idx == 0 and end_idx == 0: \n",
    "            token_labels.append(label2id[\"O\"])\n",
    "            continue\n",
    "\n",
    "        # case when token starts with whitespace\n",
    "        if text[start_idx].isspace():\n",
    "            start_idx += 1\n",
    "\n",
    "        token_labels.append(label2id[labels[start_idx]])\n",
    "\n",
    "    length = len(tokenized.input_ids)\n",
    "\n",
    "    return {\n",
    "        **tokenized,\n",
    "        \"labels\": token_labels,\n",
    "        \"length\": length,\n",
    "        \"target_num\": target_num,\n",
    "        \"group\": 1 if target_num>0 else 0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.pretrained_model_name)\n",
    "# tokenizer.save_pretrained(\"./outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DS.from_dict({\n",
    "    \"full_text\": [x[\"full_text\"] for x in data],\n",
    "    \"document\": [str(x[\"document\"]) for x in data],\n",
    "    \"tokens\": [x[\"tokens\"] for x in data],\n",
    "    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "    \"provided_labels\": [x[\"labels\"] for x in data],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds  = ds.map(tokenize, fn_kwargs={\n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"label2id\": label2id\n",
    "}, num_proc = 6)\n",
    "\n",
    "ds.class_encode_column(\"group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(test_size=0.2, seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator_fn = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = ['full_text', 'document', 'tokens', 'trailing_whitespace', 'provided_labels', 'offset_mapping', 'length', 'target_num', 'group']\n",
    "\n",
    "def get_dataset(dataset, data_type=\"train\"):\n",
    "    data = dataset[data_type]\n",
    "    data = data.remove_columns(cols_to_remove)\n",
    "    data = data.with_format(\"torch\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "def _configure_optimizer(lr, epochs, weight_decay, params):\n",
    "    \"Prepare optimizer and schedule (linear warmup and decay)\"        \n",
    "    model_optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, params), \n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "                        model_optimizer, \n",
    "                        T_0=epochs, \n",
    "                        T_mult=1, \n",
    "                        eta_min=1e-6, \n",
    "                        last_epoch=-1\n",
    "                    )\n",
    "    interval = \"epoch\"\n",
    "    return {\n",
    "    \"optimizer\": model_optimizer, \n",
    "    \"lr_scheduler\": {\n",
    "        \"scheduler\": lr_scheduler,\n",
    "        \"interval\": interval,\n",
    "        \"monitor\": \"val_loss\",\n",
    "        \"frequency\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Replace with your validation metric\n",
    "    mode='min',          # 'min' if the metric should be minimized (e.g., loss), 'max' for maximization (e.g., accuracy)\n",
    "    save_top_k=k,        # Save top k checkpoints based on the monitored metric\n",
    "    save_last=True,      # Save the last checkpoint at the end of training # Directory where the checkpoints will be saved\n",
    "    filename='{epoch}-{train_loss:.2f}'  # Checkpoint file naming pattern\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import recall_score, precision_score\n",
    "\n",
    "def compute_metrics(p, all_labels):\n",
    "    predictions, labels = p\n",
    "    predictions = torch.argmax(predictions, axis=2).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n",
    "    \n",
    "    results = {\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1': f1_score\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = AutoModelForTokenClassification.from_pretrained(\n",
    "            CFG.pretrained_model_name,\n",
    "            num_labels =len(all_labels) ,\n",
    "            id2label=id2label, label2id=label2id\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone.save_pretrained(\"./outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenFineTuner(pl.LightningModule):\n",
    "    def __init__(self, model,  hparam, dataset):\n",
    "        super(TokenFineTuner, self).__init__()\n",
    "        self.hparam = hparam\n",
    "        self.num_labels = len(id2label.keys())\n",
    "        self.model = model\n",
    "        self.ds = dataset\n",
    "        self.save_hyperparameters()\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, lm_labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "\n",
    "    def _step(self, batch):\n",
    "        labels = batch[\"labels\"]\n",
    "        output = self(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            lm_labels=labels\n",
    "            )\n",
    "    \n",
    "        return {\"loss\": output.loss, \"logits\": output.logits}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch=batch)[\"loss\"]\n",
    "        \n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self._step(batch)\n",
    "        loss = output[\"loss\"]\n",
    "        self.validation_step_outputs.append({\n",
    "            **output,\n",
    "            \"targets\": batch[\"labels\"]\n",
    "        })\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return _configure_optimizer(lr=self.hparam.learning_rate, \n",
    "                                    epochs=self.hparam.num_train_epochs,\n",
    "                                    weight_decay=self.hparam.weight_decay,\n",
    "                                    params=self.parameters())\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        data = get_dataset(self.ds)\n",
    "        dataloader = DataLoader(data, batch_size=self.hparam.train_batch_size, collate_fn=collator_fn,\n",
    "                                shuffle=True, num_workers=4)\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(self.ds, data_type=\"test\")\n",
    "        return DataLoader(val_dataset, batch_size=self.hparam.eval_batch_size, collate_fn=collator_fn, num_workers=4,shuffle=True)\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        outputs = self.validation_step_outputs\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        output_val = nn.Softmax(dim=1)(torch.cat([x['logits'] for x in outputs],dim=0))\n",
    "        target_val = torch.cat([x['targets'] for x in outputs],dim=0)\n",
    "        avg_score = compute_metrics((output_val, target_val), all_labels=list(label2id.keys()))\n",
    "        self.log(\"val_f5\", avg_score[\"f1\"],on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_precision\", avg_score[\"precision\"],on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_recall\", avg_score[\"recall\"],on_epoch=True, prog_bar=True)\n",
    "        return {'val_loss': avg_loss,'val_cmap':avg_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    output_dir=\"./\", # path to save the checkpoints\n",
    "    model_name_or_path='microsoft/deberta-v3-large',\n",
    "    tokenizer_name_or_path='microsoft/deberta-v3-large',\n",
    "    max_seq_length=256,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=1e-2,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=4,\n",
    "    num_train_epochs=100,\n",
    "    gradient_accumulation_steps=16,\n",
    "    n_gpu=1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=True, # if you want to enable 16-bit training then install apex and set this to true\n",
    "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "    max_grad_norm=1, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(**args_dict)\n",
    "model = TokenFineTuner(backbone, args, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(project=\"PIDD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    devices = 1,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    #early_stop_callback=False,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    logger=wandb_logger,\n",
    "    callbacks = [checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_checkpoint(\"./outputs/model.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
