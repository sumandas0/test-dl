{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":7659420,"sourceType":"datasetVersion","datasetId":4459964}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport copy\nimport gc\nimport os\nimport re\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport pandas as pd\nfrom spacy.lang.en import English\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom transformers import AutoModel, AutoTokenizer, AdamW, DataCollatorWithPadding\nfrom transformers.data.data_collator import DataCollatorForTokenClassification\nfrom datasets import Dataset, DatasetDict, concatenate_datasets\nimport wandb\nfrom transformers import TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\nfrom transformers import get_polynomial_decay_schedule_with_warmup,get_cosine_schedule_with_warmup,get_linear_schedule_with_warmup","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-01T17:02:47.669593Z","iopub.execute_input":"2024-04-01T17:02:47.669950Z","iopub.status.idle":"2024-04-01T17:03:13.161505Z","shell.execute_reply.started":"2024-04-01T17:02:47.669920Z","shell.execute_reply":"2024-04-01T17:03:13.160775Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-04-01 17:03:04.105314: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-01 17:03:04.105439: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-01 17:03:04.240545: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"class CFG:\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    seed = 69\n    # dataset path \n    train_dataset_path = \"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"\n    external_dataset_path_1 = \"/kaggle/input/pii-mixtral8x7b-generated-essays/mpware_mixtral8x7b_v1.1-no-i-username.json\"\n    test_dataset_path = \"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"\n    sample_submission_path = \"/kaggle/input/pii-detection-removal-from-educational-data/sample_submission.csv\"\n    save_dir=\"/kaggle/working/exp1\"\n\n    #tokenizer params\n    downsample = 0.45\n    truncation = True \n    padding = False #'max_length'\n    max_length = 1024\n    freeze_layers = 0\n    # model params\n    model_name = \"Qwen/Qwen1.5-0.5B\"\n    \n    target_cols = ['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL','O']\n\n    load_from_disk = None\n    #training params\n    learning_rate = 1e-5\n    batch_size = 2\n    epochs = 3\n\n\nseed_everything(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:19.662207Z","iopub.execute_input":"2024-04-01T17:03:19.662606Z","iopub.status.idle":"2024-04-01T17:03:19.676045Z","shell.execute_reply.started":"2024-04-01T17:03:19.662576Z","shell.execute_reply":"2024-04-01T17:03:19.675095Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"69"},"metadata":{}}]},{"cell_type":"code","source":"original_data = None\nmpware=None","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:20.169158Z","iopub.execute_input":"2024-04-01T17:03:20.169965Z","iopub.status.idle":"2024-04-01T17:03:20.173532Z","shell.execute_reply.started":"2024-04-01T17:03:20.169935Z","shell.execute_reply":"2024-04-01T17:03:20.172634Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"with Path(CFG.train_dataset_path).open(\"r\") as f:\n    original_data = json.load(f)\n\nwith Path(CFG.external_dataset_path_1).open(\"r\") as f:\n    mpware = json.load(f)\nprint(\"MPWARE's datapoints: \", len(mpware))","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:20.455943Z","iopub.execute_input":"2024-04-01T17:03:20.456570Z","iopub.status.idle":"2024-04-01T17:03:24.431131Z","shell.execute_reply.started":"2024-04-01T17:03:20.456537Z","shell.execute_reply":"2024-04-01T17:03:24.430190Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"MPWARE's datapoints:  2692\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train = pd.DataFrame(original_data)\ndf_mpware = pd.DataFrame(mpware)\ndf_mpware['document'] =  [i+30000 for i in range(len(df_mpware))]","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:24.432724Z","iopub.execute_input":"2024-04-01T17:03:24.433010Z","iopub.status.idle":"2024-04-01T17:03:24.466208Z","shell.execute_reply.started":"2024-04-01T17:03:24.432985Z","shell.execute_reply":"2024-04-01T17:03:24.465280Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_train = pd.concat([df_train,df_mpware],axis=0).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:24.467354Z","iopub.execute_input":"2024-04-01T17:03:24.467629Z","iopub.status.idle":"2024-04-01T17:03:24.488474Z","shell.execute_reply.started":"2024-04-01T17:03:24.467605Z","shell.execute_reply":"2024-04-01T17:03:24.487516Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists(CFG.save_dir):\n  os.makedirs(CFG.save_dir)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:24.490605Z","iopub.execute_input":"2024-04-01T17:03:24.490907Z","iopub.status.idle":"2024-04-01T17:03:24.495643Z","shell.execute_reply.started":"2024-04-01T17:03:24.490883Z","shell.execute_reply":"2024-04-01T17:03:24.494803Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"all_labels = [\n    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O'\n]\nid2label = {i: l for i, l in enumerate(all_labels)}\nlabel2id = {v: k for k, v in id2label.items()}\ntarget = [l for l in all_labels if l != \"O\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:24.496776Z","iopub.execute_input":"2024-04-01T17:03:24.497037Z","iopub.status.idle":"2024-04-01T17:03:24.505883Z","shell.execute_reply.started":"2024-04-01T17:03:24.497015Z","shell.execute_reply":"2024-04-01T17:03:24.505082Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def tokenize_row(example):\n    text = []\n    token_map = []\n    labels = []\n    targets = []\n    idx = 0\n    for t, l, ws in zip(example[\"tokens\"], example[\"labels\"], example[\"trailing_whitespace\"]):\n        text.append(t)\n        labels.extend([l]*len(t))\n        token_map.extend([idx]*len(t))\n\n        if l in CFG.target_cols:  \n            targets.append(1)\n        else:\n            targets.append(0)\n        \n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n            token_map.append(-1)\n        idx += 1\n\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=CFG.max_length)  # Adjust max_length if needed\n        \n    target_num = sum(targets)\n    labels = np.array(labels)\n\n    text = \"\".join(text)\n    token_labels = []\n\n    for start_idx, end_idx in tokenized.offset_mapping:\n        if start_idx == 0 and end_idx == 0: \n            token_labels.append(label2id[\"O\"])\n            continue\n        \n        if text[start_idx].isspace():\n            start_idx += 1\n        try:\n            token_labels.append(label2id[labels[start_idx]])\n        except:\n            continue\n    length = len(tokenized.input_ids)\n    \n    return {\n        \"input_ids\": tokenized.input_ids,\n        \"attention_mask\": tokenized.attention_mask,\n        \"offset_mapping\": tokenized.offset_mapping,\n        \"labels\": token_labels,\n        \"length\": length,\n        \"target_num\": target_num,\n        \"group\": 1 if target_num > 0 else 0,\n        \"token_map\": token_map,\n    }","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:24.506990Z","iopub.execute_input":"2024-04-01T17:03:24.507291Z","iopub.status.idle":"2024-04-01T17:03:24.520175Z","shell.execute_reply.started":"2024-04-01T17:03:24.507244Z","shell.execute_reply":"2024-04-01T17:03:24.519391Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\ntokenizer.save_pretrained(f'{CFG.save_dir}')","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:24.521339Z","iopub.execute_input":"2024-04-01T17:03:24.521911Z","iopub.status.idle":"2024-04-01T17:03:26.065953Z","shell.execute_reply.started":"2024-04-01T17:03:24.521878Z","shell.execute_reply":"2024-04-01T17:03:26.065006Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a778eaefe0b74323bb22ebf50a877746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12e57212a9364701810abd2a3df6b51c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef43f5a903d940c5a4f665045ce1bc0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df98477e5709483cbd72e8c1b6a7219b"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/exp1/tokenizer_config.json',\n '/kaggle/working/exp1/special_tokens_map.json',\n '/kaggle/working/exp1/vocab.json',\n '/kaggle/working/exp1/merges.txt',\n '/kaggle/working/exp1/added_tokens.json',\n '/kaggle/working/exp1/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\ndef downsample_df(train_df, percent):\n\n    train_df['is_labels'] = train_df['labels'].apply(lambda labels: any(label != 'O' for label in labels))\n    \n    true_samples = train_df[train_df['is_labels'] == True]\n    false_samples = train_df[train_df['is_labels'] == False]\n    \n    n_false_samples = int(len(false_samples) * percent)\n    downsampled_false_samples = false_samples.sample(n=n_false_samples, random_state=42)\n    \n    downsampled_df = pd.concat([true_samples, downsampled_false_samples])    \n    return downsampled_df","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:26.067215Z","iopub.execute_input":"2024-04-01T17:03:26.067577Z","iopub.status.idle":"2024-04-01T17:03:26.074299Z","shell.execute_reply.started":"2024-04-01T17:03:26.067545Z","shell.execute_reply":"2024-04-01T17:03:26.073461Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def add_token_indices(doc_tokens):\n    token_indices = list(range(len(doc_tokens)))\n    return token_indices\n\ndf_train['token_indices'] = df_train['tokens'].apply(add_token_indices)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:26.076556Z","iopub.execute_input":"2024-04-01T17:03:26.076821Z","iopub.status.idle":"2024-04-01T17:03:26.347875Z","shell.execute_reply.started":"2024-04-01T17:03:26.076797Z","shell.execute_reply":"2024-04-01T17:03:26.346790Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df_train = df_train[:10]","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:26.348967Z","iopub.execute_input":"2024-04-01T17:03:26.349306Z","iopub.status.idle":"2024-04-01T17:03:26.355320Z","shell.execute_reply.started":"2024-04-01T17:03:26.349257Z","shell.execute_reply":"2024-04-01T17:03:26.354318Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_train = downsample_df(df_train, CFG.downsample)\nds = Dataset.from_pandas(df_train)\nds = ds.map(\n          tokenize_row,\n          batched=False,\n          num_proc=2,\n          desc=\"Tokenizing\",\n      )","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:26.356582Z","iopub.execute_input":"2024-04-01T17:03:26.356939Z","iopub.status.idle":"2024-04-01T17:03:26.918348Z","shell.execute_reply.started":"2024-04-01T17:03:26.356908Z","shell.execute_reply":"2024-04-01T17:03:26.917341Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"    ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing #0:   0%|          | 0/5 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caf4dd866bec4ce4908c5f2f7dd0cd8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing #1:   0%|          | 0/5 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c69967cd3f6e4e1b913cbc1928c11ab9"}},"metadata":{}}]},{"cell_type":"code","source":"import random\n\nclass LSTMHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, n_layers):\n        super().__init__()\n        self.lstm = nn.LSTM(in_features,\n                            hidden_dim,\n                            n_layers,\n                            batch_first=True,\n                            bidirectional=True,\n                            dropout=0.1)\n        self.out_features = hidden_dim\n\n    def forward(self, x):\n        self.lstm.flatten_parameters()\n        hidden, (_, _) = self.lstm(x)\n        out = hidden\n        return out\n\n    \nclass PIIModel(pl.LightningModule):\n    def __init__(self,config):\n        super().__init__()\n        self.cfg = config\n        self.model_config = AutoConfig.from_pretrained(\n            config.model_name,\n        )\n\n        hidden_dropout_prob: float = 0.1\n        layer_norm_eps: float = 1e-7\n        self.model_config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n\n        self.transformers_model = AutoModel.from_pretrained(self.cfg.model_name,config=self.model_config)\n        self.head = LSTMHead(in_features=self.model_config.hidden_size, hidden_dim=self.model_config.hidden_size//2, n_layers=1)\n\n        self.output = nn.Linear(self.model_config.hidden_size, len(self.cfg.target_cols))\n        \n        if self.cfg.freeze_layers>0:\n            print(f'Freezing {self.cfg.freeze_layers} layers.')\n            for layer in self.transformers_model.longformer.encoder.layer[:self.cfg.freeze_layers]:\n                for param in layer.parameters():\n                    param.requires_grad = False\n\n\n        self.loss_function = nn.CrossEntropyLoss(reduction='mean',ignore_index=-100) \n        self.validation_step_outputs = []\n\n    def forward(self, input_ids, attention_mask,train):\n        \n        transformer_out = self.transformers_model(input_ids,attention_mask = attention_mask)\n        sequence_output = transformer_out.last_hidden_state\n        sequence_output = self.head(sequence_output)\n        logits = self.output(sequence_output)\n\n        return (logits, _)\n    \n\n    def training_step(self,batch,batch_idx):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        target = batch['labels'] \n\n        outputs = self(input_ids,attention_mask,train=True)\n        output = outputs[0]\n        loss = self.loss_function(output.view(-1,len(self.cfg.target_cols)), target.view(-1))\n        \n        self.log('train_loss', loss , prog_bar=True)\n        return {'loss': loss}\n    \n    def train_epoch_end(self,outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        print(f'epoch {trainer.current_epoch} training loss {avg_loss}')\n        return {'train_loss': avg_loss} \n        \n    def train_dataloader(self):\n        return self._train_dataloader \n\n    def get_optimizer_params(self, encoder_lr, decoder_lr, weight_decay=0.0):\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {'params': [p for n, p in self.transformers_model.named_parameters() if not any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': weight_decay},\n            {'params': [p for n, p in self.transformers_model.named_parameters() if any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': 0.0},\n            {'params': [p for n, p in self.named_parameters() if \"transformers_model\" not in n],\n             'lr': decoder_lr, 'weight_decay': 0.0}\n        ]\n        return optimizer_parameters\n\n    def configure_optimizers(self):\n        optimizer = AdamW(self.parameters(), lr = self.cfg.learning_rate)\n\n        epoch_steps = self.cfg.data_length\n        batch_size = self.cfg.batch_size\n\n        warmup_steps = 0.05 * epoch_steps // batch_size\n        training_steps = self.cfg.epochs * epoch_steps // batch_size\n        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, training_steps, num_cycles=0.5)\n        \n        lr_scheduler_config = {\n                'scheduler': scheduler,\n                'interval': 'step',\n                'frequency': 1,\n            }\n\n        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler_config}","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:26.921111Z","iopub.execute_input":"2024-04-01T17:03:26.921515Z","iopub.status.idle":"2024-04-01T17:03:26.943726Z","shell.execute_reply.started":"2024-04-01T17:03:26.921476Z","shell.execute_reply":"2024-04-01T17:03:26.942843Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=512)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:27.683976Z","iopub.execute_input":"2024-04-01T17:03:27.684346Z","iopub.status.idle":"2024-04-01T17:03:27.689050Z","shell.execute_reply.started":"2024-04-01T17:03:27.684317Z","shell.execute_reply":"2024-04-01T17:03:27.688052Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\ntrain_ds = ds\ntrain_ds = train_ds.remove_columns([c for c in train_ds.column_names if c not in keep_cols])\nCFG.data_length = len(train_ds)\nCFG.len_token = len(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:27.976033Z","iopub.execute_input":"2024-04-01T17:03:27.976379Z","iopub.status.idle":"2024-04-01T17:03:28.013982Z","shell.execute_reply.started":"2024-04-01T17:03:27.976352Z","shell.execute_reply":"2024-04-01T17:03:28.013218Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print('Dataset Loaded....')\nprint((train_ds[0].keys()))\nprint(\"Generating Train DataLoader\")\ntrain_dataloader = DataLoader(train_ds, batch_size = CFG.batch_size, shuffle = True, num_workers= 4, pin_memory=False,collate_fn = collator)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:55.182309Z","iopub.execute_input":"2024-04-01T17:03:55.183172Z","iopub.status.idle":"2024-04-01T17:03:55.190923Z","shell.execute_reply.started":"2024-04-01T17:03:55.183138Z","shell.execute_reply":"2024-04-01T17:03:55.189963Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Dataset Loaded....\ndict_keys(['labels', 'input_ids', 'attention_mask'])\nGenerating Train DataLoader\n","output_type":"stream"}]},{"cell_type":"code","source":"early_stop_callback = EarlyStopping(monitor=\"train_loss\", min_delta=0.00, patience=8, verbose= True, mode=\"min\")\ncheckpoint_callback = ModelCheckpoint(monitor='train_loss',\n                                          dirpath= CFG.save_dir,\n                                      save_top_k=1,\n                                      save_last= True,\n                                      save_weights_only=True,\n                                      verbose= True,\n                                      mode='min')\n    \nprint(\"Model Creation\")","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:03:55.602243Z","iopub.execute_input":"2024-04-01T17:03:55.602859Z","iopub.status.idle":"2024-04-01T17:03:55.621466Z","shell.execute_reply.started":"2024-04-01T17:03:55.602824Z","shell.execute_reply":"2024-04-01T17:03:55.620532Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Model Creation\n","output_type":"stream"}]},{"cell_type":"code","source":"model = PIIModel(CFG)\n# model.load_state_dict(torch.load('/home/nischay/PID/nbs/outputs2/exp12_baseline_debv3base_1024_extv1/ckeckpoint_0-v2.ckpt','cpu')['state_dict'])\ntrainer = Trainer(max_epochs= CFG.epochs,\n                      deterministic=False,\n                      accumulate_grad_batches=1, \n                      devices=[0],\n                      precision=16, \n                      accelerator=CFG.device ,\n                      callbacks=[checkpoint_callback,early_stop_callback]) ","metadata":{"execution":{"iopub.status.busy":"2024-04-01T16:59:54.002826Z","iopub.execute_input":"2024-04-01T16:59:54.003640Z","iopub.status.idle":"2024-04-01T17:00:13.580830Z","shell.execute_reply.started":"2024-04-01T16:59:54.003609Z","shell.execute_reply":"2024-04-01T17:00:13.579851Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7fb5d5f01e94b9c8fbe86ef33333b84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da1e138b44df49509bc615cb03dce85c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n/opt/conda/lib/python3.10/site-packages/lightning_fabric/connector.py:563: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n","output_type":"stream"}]},{"cell_type":"code","source":"CFG.data_length = len(train_ds)\ntrainer.fit(model,train_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T17:00:16.538326Z","iopub.execute_input":"2024-04-01T17:00:16.539146Z","iopub.status.idle":"2024-04-01T17:01:25.692375Z","shell.execute_reply.started":"2024-04-01T17:00:16.539112Z","shell.execute_reply":"2024-04-01T17:01:25.691347Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /kaggle/working/exp1 exists and is not empty.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2c2b3a9957c49fb9d45b285b9892051"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}